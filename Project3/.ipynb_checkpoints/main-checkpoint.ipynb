{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import math, getopt, sys\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from skimage.io import imshow\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import exposure, measure\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import sobel, rank, gaussian, median, threshold_adaptive\n",
    "from skimage.morphology import disk, watershed, square\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "from skimage import morphology\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_limit = 0.01\n",
    "bins = 15\n",
    "\n",
    "sigma_for_images = 1\n",
    "\n",
    "use_gaussian = 1\n",
    "use_median = 0\n",
    "\n",
    "use_equalize_hist = 0\n",
    "use_adapthist = 1\n",
    "use_rescale_intensity = 0\n",
    "\n",
    "save_images = 0\n",
    "save_histograms = 0\n",
    "\n",
    "use_pca_for_segments = 1\n",
    "use_pca_for_learning = 0\n",
    "use_pca_before_learning = 1\n",
    "\n",
    "n_components_for_2D_segments = 5\n",
    "n_components_for_3D_segments = 1\n",
    "n_components_before_learning = 20\n",
    "\n",
    "use_3D = 0\n",
    "\n",
    "x_parts, y_parts, z_parts = 5, 5, 5\n",
    "\n",
    "cv_inner_folds = 10\n",
    "cv_outer_folds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate n_arr (almost) equal-sized arrays from arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(arr, n_arr):\n",
    "    n_elems = int(math.ceil(len(arr)/n_arr))\n",
    "    return [arr[i:i + n_elems] for i in range(0, len(arr), n_elems)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load images from file being in defined path, deleting all zero vectors from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, file):\n",
    "    \n",
    "    filepath = os.path.join(path, file)\n",
    "    img = nib.load(filepath)\n",
    "    img_data = img.get_data()\n",
    "    img_data = img_data[:,:,:,0]\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_2D_image(img_data):\n",
    "    \n",
    "    img_data = np.delete(img_data, x_zero, axis=0)\n",
    "    img_data = np.delete(img_data, y_zero, axis=1)\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_3D_image(img_data):\n",
    "    \n",
    "    img_data = np.delete(img_data, x_zero, axis=0)\n",
    "    img_data = np.delete(img_data, y_zero, axis=1)\n",
    "    img_data = np.delete(img_data, z_zero, axis=2)\n",
    "    \n",
    "    x_res = img_data.shape[0] - img_data.shape[0] % x_parts\n",
    "    y_res = img_data.shape[1] - img_data.shape[1] % y_parts\n",
    "    z_res = img_data.shape[2] - img_data.shape[2] % z_parts\n",
    "    img_data = img_data[: x_res, : y_res, : z_res]\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2D_image(img_data):\n",
    "    \n",
    "    img_data = np.uint16(img_data)\n",
    "    mask = np.zeros_like(img_data)\n",
    "    mask = img_data == 0\n",
    "    \n",
    "    if use_gaussian:\n",
    "        img_data = gaussian(img_data, sigma_for_images)\n",
    "    if use_median:\n",
    "        img_data = median(img_data, square(3))\n",
    "    if use_rescale_intensity:\n",
    "        p2, p98 = np.percentile(img_data, (2, 98))\n",
    "        img_data = exposure.rescale_intensity(img_data, in_range=(p2, p98))\n",
    "    if use_equalize_hist:\n",
    "        img_data = exposure.equalize_hist(img_data)\n",
    "    if use_adapthist:\n",
    "        img_data = exposure.equalize_adapthist(img_data, clip_limit=clip_limit)\n",
    "    \n",
    "    img_data[mask] = 0\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a slice from 3D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_image(img_data, axis='z', depth=90, save=False, file='test.nii'):\n",
    "    \n",
    "    if axis=='x':\n",
    "        img_data = img_data[depth,:,:]\n",
    "    elif axis=='y':\n",
    "        img_data = img_data[:,depth,:]\n",
    "    elif axis=='z':\n",
    "        img_data = img_data[:,:,depth]\n",
    "    \n",
    "    if save==True:\n",
    "        filename = file[0:-4] + \"_\" + axis + \"_\" + str(depth) + \".png\"\n",
    "        savepath = os.path.join(\"images\", filename)\n",
    "        imsave(savepath, img_data)\n",
    "\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segmentation(img_data, file):\n",
    "    \n",
    "    global segmentation\n",
    "    \n",
    "    img_data = np.uint16(img_data)\n",
    "    mask = np.zeros_like(img_data)\n",
    "    mask = img_data == 0\n",
    "    \n",
    "    img_data = gaussian(img_data, sigma_for_segmentation)\n",
    "#     img_data = ndi.gaussian_filter(img_data, sigma)\n",
    "    img_data = exposure.equalize_hist(img_data)\n",
    "    img_data[mask] = 0\n",
    "    \n",
    "    if use_disk:\n",
    "        markers = rank.gradient(img_data, disk(1)) < 1\n",
    "    else:\n",
    "        markers = rank.gradient(img_data, square(3)) < 1\n",
    "    \n",
    "    markers = morphology.remove_small_objects(markers, min_object_size)\n",
    "    markers = ndi.label(markers)[0]\n",
    "    elevation_map = sobel(img_data)\n",
    "    segmentation = watershed(elevation_map, markers, mask=~mask)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img_data, cmap='gray')\n",
    "    plt.imshow(segmentation, cmap='spectral', alpha=0.4)\n",
    "\n",
    "    savepath = os.path.join(\"images\", file[0:-4] + \"_\" + date + \".png\")\n",
    "    plt.savefig(savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate 2D histogram for a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_2D_segm_histograms(src_path, file):\n",
    "    \n",
    "    img_data = load_image(src_path, file)\n",
    "    img_data = slice_image(img_data, axis=axis, depth=depth)\n",
    "    img_data = cut_2D_image(img_data)\n",
    "    img_data = preprocess_2D_image(img_data)\n",
    "    \n",
    "    if save_images:\n",
    "        plt.figure()\n",
    "        plt.imshow(img_data, cmap='gray')\n",
    "        plt.imshow(segmentation, cmap='spectral', alpha=0.4)\n",
    "\n",
    "        savepath = os.path.join(\"images\", file[0:-4] + \".png\")\n",
    "        plt.savefig(savepath)\n",
    "    \n",
    "    labels = np.unique(segmentation)\n",
    "    labels = np.delete(labels, np.where(labels == 0))\n",
    "    part_array = np.array([])\n",
    "    \n",
    "    for i in labels:\n",
    "        indeces = segmentation == i\n",
    "        img_part = img_data[indeces]\n",
    "        hist, bin_edges = np.histogram(img_part, range=(0,1), bins=bins)\n",
    "        part_array = np.hstack((part_array, hist))\n",
    "        part_array = np.append(part_array, np.mean(img_part))\n",
    "        part_array = np.append(part_array, np.var(img_part))\n",
    "        part_array = np.append(part_array, np.median(img_part))\n",
    "        \n",
    "        if save_histograms:\n",
    "            plt.figure()\n",
    "            plt.hist(img_part, range=(clip_limit,1-clip_limit), bins=bins)\n",
    "            savepath = os.path.join(\"images\", \"histograms\", file[0:-4] + \"_hist_\" + str(i) + \".png\")\n",
    "            plt.savefig(savepath)\n",
    "        \n",
    "    return part_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_configuration():\n",
    "    \n",
    "    global date, X_filename, X_test_filename, src_train_path, src_test_path, train_names, test_names\n",
    "    \n",
    "    today = datetime.datetime.today()\n",
    "    date_format = \"%d%m%y_%H%M%S\"\n",
    "    date = today.strftime(date_format)\n",
    "    \n",
    "    src_train_path = os.path.join(os.getcwd(), \"..\", \"data\", \"set_train\")\n",
    "    src_test_path = os.path.join(os.getcwd(), \"..\", \"data\", \"set_test\")\n",
    "    \n",
    "    train_filepaths = os.path.join(src_train_path, \"*.nii\")\n",
    "    train_paths = (glob.glob(train_filepaths))\n",
    "    train_names = [os.path.basename(x) for x in train_paths]\n",
    "    train_names = natsorted(train_names)\n",
    "\n",
    "    test_filepaths = os.path.join(src_test_path, \"*.nii\")\n",
    "    test_paths = (glob.glob(test_filepaths))\n",
    "    test_names = [os.path.basename(x) for x in test_paths]\n",
    "    test_names = natsorted(test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate histograms for train and test data. The same number of bins is used for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_2D_histogram_arrays():\n",
    "\n",
    "    global x_zero, y_zero\n",
    "    \n",
    "    file = train_names[image_to_generate_segments]\n",
    "    img_data = load_image(src_train_path, file)\n",
    "    img_data = slice_image(img_data, axis=axis, depth=depth)\n",
    "    \n",
    "    x_zero = np.where(np.all(img_data==0, axis=1) == 1)\n",
    "    y_zero = np.where(np.all(img_data==0, axis=0) == 1)\n",
    "    \n",
    "    img_data = cut_2D_image(img_data)\n",
    "    find_segmentation(img_data, file)\n",
    "    \n",
    "    for i, file in enumerate(train_names):\n",
    "        \n",
    "        part_array = count_2D_segm_histograms(src_train_path, file)\n",
    "        \n",
    "        if i == 0:\n",
    "            X = part_array\n",
    "        else:\n",
    "            X = np.vstack((X, part_array))\n",
    "            \n",
    "    for i, file in enumerate(test_names):\n",
    "        \n",
    "        part_array = count_2D_segm_histograms(src_test_path, file)\n",
    "        \n",
    "        if i == 0:\n",
    "            X_test = part_array\n",
    "        else:\n",
    "            X_test = np.vstack((X_test, part_array))\n",
    "            \n",
    "    if use_pca_for_segments:\n",
    "    \n",
    "        scl = StandardScaler()\n",
    "        X = scl.fit_transform(X)\n",
    "        X_test = scl.transform(X_test)\n",
    "\n",
    "        labels = np.unique(segmentation)\n",
    "        labels = np.delete(labels, np.where(labels == 0))\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            X_temp = X[:,range(i*(bins+3), (i+1)*(bins+3))]\n",
    "            X_test_temp = X_test[:,range(i*(bins+3), (i+1)*(bins+3))]\n",
    "            pca = PCA(n_components=n_components_for_2D_segments)\n",
    "            X_temp = pca.fit_transform(X_temp)\n",
    "            X_test_temp = pca.transform(X_test_temp)\n",
    "\n",
    "            if i==0:\n",
    "                X_new = X_temp\n",
    "                X_test_new = X_test_temp\n",
    "            else:\n",
    "                X_new = np.hstack((X_new, X_temp))\n",
    "                X_test_new = np.hstack((X_test_new, X_test_temp))\n",
    "\n",
    "        X = X_new\n",
    "        X_test = X_test_new\n",
    "        \n",
    "    return X.astype(np.float64), X_test.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_3D_image(img_data):\n",
    "    \n",
    "    img_data = np.uint16(img_data)\n",
    "    mask = np.zeros_like(img_data)\n",
    "    mask = img_data == 0\n",
    "#     img_shape = img_data.shape\n",
    "    \n",
    "    if use_gaussian:\n",
    "        img_data = gaussian(img_data, sigma_for_images, multichannel=False)\n",
    "    if use_median:\n",
    "        img_data = median(img_data, square(3))\n",
    "        \n",
    "    img_data[mask] = 0\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_3D_histogram_arrays():\n",
    "\n",
    "    global x_zero, y_zero, z_zero, x_pix, y_pix, z_pix\n",
    "    \n",
    "    file = train_names[image_to_generate_segments]\n",
    "    img_data = load_image(src_train_path, file)\n",
    "    \n",
    "    x_zero = np.where(np.all(img_data==0, axis=(1,2)) == 1)\n",
    "    y_zero = np.where(np.all(img_data==0, axis=(0,2)) == 1)\n",
    "    z_zero = np.where(np.all(img_data==0, axis=(0,1)) == 1)\n",
    "    \n",
    "    img_data = cut_3D_image(img_data)\n",
    "    (x_dim, y_dim, z_dim) = img_data.shape\n",
    "    x_pix, y_pix, z_pix = chunks(range(x_dim),x_parts), chunks(range(y_dim),y_parts), chunks(range(z_dim),z_parts)\n",
    "    \n",
    "    for i, file in enumerate(train_names):\n",
    "        \n",
    "        part_array = count_3D_histograms(src_train_path, file)\n",
    "        \n",
    "        if i == 0:\n",
    "            X = part_array\n",
    "        else:\n",
    "            X = np.vstack((X, part_array))\n",
    "            \n",
    "    for i, file in enumerate(test_names):\n",
    "        \n",
    "        part_array = count_3D_histograms(src_test_path, file)\n",
    "        \n",
    "        if i == 0:\n",
    "            X_test = part_array\n",
    "        else:\n",
    "            X_test = np.vstack((X_test, part_array))\n",
    "            \n",
    "    if use_pca_for_segments:\n",
    "    \n",
    "        scl = StandardScaler()\n",
    "        X = scl.fit_transform(X)\n",
    "        X_test = scl.transform(X_test)\n",
    "\n",
    "        for i in range(x_parts*x_parts*z_parts):\n",
    "            X_temp = X[:,range(i*(bins+3), (i+1)*(bins+3))]\n",
    "            X_test_temp = X_test[:,range(i*(bins+3), (i+1)*(bins+3))]\n",
    "            pca = PCA(n_components=n_components_for_3D_segments)\n",
    "            X_temp = pca.fit_transform(X_temp)\n",
    "            X_test_temp = pca.transform(X_test_temp)\n",
    "\n",
    "            if i==0:\n",
    "                X_new = X_temp\n",
    "                X_test_new = X_test_temp\n",
    "            else:\n",
    "                X_new = np.hstack((X_new, X_temp))\n",
    "                X_test_new = np.hstack((X_test_new, X_test_temp))\n",
    "\n",
    "        X = X_new\n",
    "        X_test = X_test_new\n",
    "        \n",
    "    return X.astype(np.float64), X_test.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_3D_histograms(src_path, file):\n",
    "    \n",
    "    img_data = load_image(src_path, file)\n",
    "    img_data = cut_3D_image(img_data)\n",
    "    img_data = preprocess_3D_image(img_data)\n",
    "    \n",
    "    part_array = np.array([])\n",
    "    \n",
    "    for j in range(x_parts):\n",
    "        x_range = x_pix[j]\n",
    "\n",
    "        for k in range(y_parts):\n",
    "            y_range = y_pix[k]\n",
    "\n",
    "            for l in range (z_parts):\n",
    "                z_range = z_pix[l]\n",
    "\n",
    "                indeces = np.ix_(x_range, y_range, z_range)\n",
    "                img_part = img_data[indeces]\n",
    "                \n",
    "                hist, bin_edges = np.histogram(img_part, range=(clip_limit,1-clip_limit), bins=bins)\n",
    "\n",
    "                part_array = np.hstack((part_array, hist))\n",
    "                part_array = np.append(part_array, np.mean(img_part))\n",
    "                part_array = np.append(part_array, np.var(img_part))\n",
    "                part_array = np.append(part_array, np.median(img_part))\n",
    "        \n",
    "    return part_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X_train, y_train, X_valid, y_valid, X_test, use_pca=0):\n",
    "\n",
    "    clfs = []\n",
    "    \n",
    "    param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    param_grid = {\n",
    "        'clf__C': param_range, \n",
    "        'clf__gamma': param_range, \n",
    "        'clf__kernel': ['linear', 'rbf'],\n",
    "        'clf__class_weight' : [None, 'balanced']\n",
    "    }\n",
    "    \n",
    "    if use_pca:\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), ('pca', PCA()), ('clf', SVC(random_state=1))])\n",
    "        param_grid['pca__n_components'] = [5, 10, 20]\n",
    "    else:\n",
    "        pipe_svc = Pipeline([('clf', SVC(random_state=1, probability=True))])\n",
    "        \n",
    "    clf_svc = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, cv=cv_inner_folds, n_jobs=-1)\n",
    "    \n",
    "    clfs.append(clf_svc.fit(X_train, y_train))\n",
    "#     scores_svc = cross_val_score(clf_svc, X_train, y_train, cv=cv_outer_folds, n_jobs=1)\n",
    "    \n",
    "    param_grid = {\n",
    "        'rfc__n_estimators': [10, 50, 100, 200],\n",
    "        'rfc__max_features': [None, 'auto', 'log2'],\n",
    "        'rfc__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        'rfc__bootstrap' : [False, True]\n",
    "    }\n",
    "\n",
    "    if use_pca:\n",
    "        pipe_rfc = Pipeline([('scl', StandardScaler()), ('pca', PCA()), ('rfc', RandomForestClassifier(n_jobs=-1))])\n",
    "        param_grid['pca__n_components'] = [5, 10, 20]\n",
    "    else:\n",
    "        pipe_rfc = Pipeline([('rfc', RandomForestClassifier(n_jobs=-1))])\n",
    "\n",
    "    clf_rfc = GridSearchCV(estimator=pipe_rfc, param_grid=param_grid, cv=cv_inner_folds, n_jobs=-1)\n",
    "\n",
    "    clfs.append(clf_rfc.fit(X_train, y_train))\n",
    "#     scores_rfc = cross_val_score(clf_rfc, X_train, y_train, cv=cv_outer_folds, n_jobs=1) \n",
    "    \n",
    "    param_grid = {\n",
    "        'bc__n_estimators': [10, 50, 100, 200],\n",
    "        'bc__max_features': [5, 10, 20],\n",
    "        'bc__bootstrap' : [True, False],\n",
    "        'bc__bootstrap_features' : [True, False]\n",
    "    }\n",
    "\n",
    "    if use_pca:\n",
    "        pipe_bc = Pipeline([('scl', StandardScaler()), ('pca', PCA()), ('bc', BaggingClassifier())])\n",
    "        param_grid['pca__n_components'] = [5, 10, 20]\n",
    "    else:\n",
    "        pipe_bc = Pipeline([('bc', BaggingClassifier())])\n",
    "\n",
    "    clf_bc = GridSearchCV(estimator=pipe_bc, param_grid=param_grid, cv=cv_inner_folds, n_jobs=-1)\n",
    "\n",
    "    clfs.append(clf_bc.fit(X_train, y_train))\n",
    "\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_multilabel(X_train, y_train, X_valid, y_valid, X_test, use_pca=0):\n",
    "\n",
    "    clfs = []\n",
    "    \n",
    "    \n",
    "    param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    param_grid = {\n",
    "        'clf__estimator__C': param_range, \n",
    "        'clf__estimator__gamma': param_range, \n",
    "        'clf__estimator__kernel': ['linear', 'rbf'],\n",
    "        'clf__estimator__class_weight' : [None, 'balanced']\n",
    "    }\n",
    "    \n",
    "    if use_pca:\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), ('pca', PCA()), ('clf', OneVsOneClassifier(SVC(random_state=1, probability=True)))])\n",
    "        param_grid['pca__n_components'] = [5, 10, 20]\n",
    "    else:\n",
    "        pipe_svc = Pipeline([('clf', OneVsRestClassifier(SVC(random_state=1, probability=True)))])\n",
    "        \n",
    "    clf_svc = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, cv=cv_inner_folds, n_jobs=-1)\n",
    "    \n",
    "    clfs.append(clf_svc.fit(X_train, y_train))\n",
    "    \n",
    "    \n",
    "    param_grid = {\n",
    "        'rfc__n_estimators': [10, 50, 100, 200],\n",
    "        'rfc__max_features': [None, 'auto', 'log2'],\n",
    "        'rfc__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        'rfc__bootstrap' : [False, True]\n",
    "    }\n",
    "\n",
    "    if use_pca:\n",
    "        pipe_rfc = Pipeline([('scl', StandardScaler()), ('pca', PCA()), ('rfc', RandomForestClassifier(n_jobs=-1))])\n",
    "        param_grid['pca__n_components'] = [5, 10, 20]\n",
    "    else:\n",
    "        pipe_rfc = Pipeline([('rfc', RandomForestClassifier(n_jobs=-1))])\n",
    "\n",
    "    clf_rfc = GridSearchCV(estimator=pipe_rfc, param_grid=param_grid, cv=cv_inner_folds, n_jobs=-1)\n",
    "\n",
    "    clfs.append(clf_rfc.fit(X_train, y_train))\n",
    "    \n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating X (all training samples derived from given training images), X_test (test samples derived from given test images) and y (targets for given training images) matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_configuration()\n",
    "\n",
    "y = pd.read_csv(\"targets.csv\", header=None).values\n",
    "\n",
    "#####################################################\n",
    "\n",
    "axis = 'y'\n",
    "depth = 100\n",
    "image_to_generate_segments = 112\n",
    "sigma_for_segmentation = 1.5\n",
    "use_disk = True\n",
    "min_object_size = 2\n",
    "\n",
    "X, X_test = calculate_2D_histogram_arrays()\n",
    "\n",
    "#####################################################\n",
    "\n",
    "axis = 'x'\n",
    "depth = 90\n",
    "image_to_generate_segments = 162\n",
    "sigma_for_segmentation = 1.2\n",
    "use_disk = True\n",
    "min_object_size = 4\n",
    "\n",
    "X_temp, X_test_temp = calculate_2D_histogram_arrays()\n",
    "X = np.hstack((X, X_temp))\n",
    "X_test = np.hstack((X_test, X_test_temp))\n",
    "\n",
    "#####################################################\n",
    "\n",
    "axis = 'z'\n",
    "depth = 92\n",
    "image_to_generate_segments = 150\n",
    "sigma_for_segmentation = 4\n",
    "use_disk = True\n",
    "min_object_size = 2\n",
    "\n",
    "X_temp, X_test_temp = calculate_2D_histogram_arrays()\n",
    "X = np.hstack((X, X_temp))\n",
    "X_test = np.hstack((X_test, X_test_temp))\n",
    "\n",
    "#####################################################\n",
    "\n",
    "if use_3D:\n",
    "    X_temp, X_test_temp = calculate_3D_histogram_arrays()\n",
    "    X = np.hstack((X, X_temp))\n",
    "    X_test = np.hstack((X_test, X_test_temp))\n",
    "\n",
    "#####################################################\n",
    "\n",
    "if use_pca_before_learning:\n",
    "    scl = StandardScaler()\n",
    "    X = scl.fit_transform(X)\n",
    "    X_test = scl.transform(X_test)\n",
    "    pca = PCA(n_components=n_components_before_learning)\n",
    "    X = pca.fit_transform(X)\n",
    "    X_test = pca.transform(X_test)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_age = y_train[:,1:2]\n",
    "y_valid_age = y_valid[:,1:2]\n",
    "\n",
    "clfs_age = learn(X_train, y_train_age.ravel(), X_valid, y_valid_age.ravel(), X_test, use_pca=use_pca_for_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred_age = np.zeros((X_valid.shape[0], len(clfs_age)))\n",
    "y_test_pred_age = np.zeros((X_test.shape[0], len(clfs_age)))\n",
    "\n",
    "for i, clf in enumerate(clfs_age):\n",
    "\n",
    "    y_train_pred_age = clf.predict(X_train)\n",
    "    y_valid_pred_age_ = clf.predict(X_valid)\n",
    "\n",
    "    y_valid_pred_age[:,i] = clf.predict_proba(X_valid)[:,1]\n",
    "    y_test_pred_age[:,i] = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "    train_error = hamming_loss(y_train_age, y_train_pred_age)\n",
    "    valid_error = hamming_loss(y_valid_age, y_valid_pred_age_)\n",
    "\n",
    "    train_confusion_array = confusion_matrix(y_train_age, y_train_pred_age)\n",
    "    valid_confusion_array = confusion_matrix(y_valid_age, y_valid_pred_age_)\n",
    "\n",
    "    print(clf.best_score_)\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(train_error)\n",
    "    print(valid_error)\n",
    "\n",
    "    print(train_confusion_array)\n",
    "    print(valid_confusion_array)\n",
    "\n",
    "y_valid_pred_age = np.round(np.mean(y_valid_pred_age, axis=1)).astype(int)\n",
    "y_valid_pred_age = np.reshape(y_valid_pred_age, (y_valid_pred_age.size,1))\n",
    "\n",
    "y_test_pred_age = np.round(np.mean(y_test_pred_age, axis=1)).astype(int)\n",
    "y_test_pred_age = np.reshape(y_test_pred_age, (y_test_pred_age.size,1))\n",
    "\n",
    "valid_error = hamming_loss(y_valid_age, y_valid_pred_age)\n",
    "valid_confusion_array = confusion_matrix(y_valid_age, y_valid_pred_age)\n",
    "\n",
    "print(valid_error)\n",
    "print(valid_confusion_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sick = y_train[:,2:3]\n",
    "y_valid_sick = y_valid[:,2:3]\n",
    "y_train_sex = y_train[:,0:1]\n",
    "y_valid_sex = y_valid[:,0:1]\n",
    "\n",
    "X_train_new = np.hstack((X_train, y_train_age))\n",
    "X_valid_new = np.hstack((X_valid, y_valid_pred_age))\n",
    "X_test_new = np.hstack((X_test, y_test_pred_age))\n",
    "\n",
    "clfs = learn_multilabel(X_train_new, y_train, X_valid_new, y_valid, X_test_new, use_pca=use_pca_for_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = np.zeros((X_valid.shape[0], 3, len(clfs)))\n",
    "y_test_pred = np.zeros((X_test.shape[0], 3, len(clfs)))\n",
    "\n",
    "for i, clf in enumerate(clfs):\n",
    "\n",
    "    y_train_pred = clf.predict(X_train_new)\n",
    "    y_valid_pred_ = clf.predict(X_valid_new)\n",
    "    \n",
    "    train_error = hamming_loss(y_train, y_train_pred)\n",
    "    valid_error = hamming_loss(y_valid, y_valid_pred_)\n",
    "\n",
    "    print(clf.best_score_)\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(train_error)\n",
    "    print(valid_error)\n",
    "\n",
    "for j in range(3):\n",
    "    y_valid_pred[:,j,0] = clfs[0].predict_proba(X_valid_new)[:,j]\n",
    "    y_test_pred[:,j,0] = clfs[0].predict_proba(X_test_new)[:,j]\n",
    "    \n",
    "    y_valid_pred[:,j,1] = clfs[1].predict_proba(X_valid_new)[j][:,1]\n",
    "    y_test_pred[:,j,1] = clfs[1].predict_proba(X_test_new)[j][:,1]\n",
    "    \n",
    "y_valid_pred = np.round(np.mean(y_valid_pred, axis=2)).astype(int)\n",
    "y_test_pred = np.round(np.mean(y_test_pred, axis=2)).astype(int)\n",
    "\n",
    "print(y_test_pred)\n",
    "\n",
    "valid_error = hamming_loss(y_valid, y_valid_pred)\n",
    "\n",
    "print(valid_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline with standarizing data and classifying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_error = hamming_loss(y_valid, y_valid_pred)\n",
    "\n",
    "print(valid_error)\n",
    "\n",
    "parameters = {\n",
    "    'axis' : axis,\n",
    "    'depth' : depth,\n",
    "    'clip_limit' : clip_limit,\n",
    "    'bins' : bins,\n",
    "    'image_to_generate_segments' : image_to_generate_segments,\n",
    "\n",
    "    'use_gaussian' : use_gaussian,\n",
    "    'use_median' : use_median,\n",
    "    \n",
    "    'sigma_for_segmentation' : sigma_for_segmentation,\n",
    "    'sigma_for_images' : sigma_for_images,\n",
    "\n",
    "    'use_equalize_hist' : use_equalize_hist,\n",
    "    'use_adapthist' : use_adapthist,\n",
    "    'use_rescale_intensity' : use_rescale_intensity,\n",
    "\n",
    "    'save_images' : save_images,\n",
    "    'save_histograms' : save_histograms,\n",
    "\n",
    "    'use_pca_for_segments' : use_pca_for_segments,\n",
    "    'use_pca_for_learning' : use_pca_for_learning,\n",
    "    'use_pca_before_learning' : use_pca_before_learning,\n",
    "    \n",
    "    'n_components_for_2D_segments' : n_components_for_2D_segments,\n",
    "    'n_components_for_3D_segments' : n_components_for_3D_segments,\n",
    "    'n_components_before_learning' : n_components_before_learning,\n",
    "    \n",
    "    'cv_inner_folds' : cv_inner_folds\n",
    "}\n",
    "\n",
    "savepath = os.path.join(\"results\", \"output_\" + date + \".txt\")\n",
    "with open(savepath, \"w\") as text_file:\n",
    "    text_file.write(\"Date: \" + str(date) + \"\\n\")\n",
    "    text_file.write(\"Valid error: \" + str(valid_error) + \"\\n\")\n",
    "    text_file.write(\"Parameters: \" + str(parameters) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving scores obtained from original test images to produce file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_test_pred.ravel()\n",
    "\n",
    "nr = np.arange(0,414)\n",
    "label = np.array(138*[\"gender\", \"age\", \"health\"])\n",
    "sample = np.arange(0,138)\n",
    "sample = np.dstack((sample, sample, sample))\n",
    "sample = sample.ravel()\n",
    "label = label.ravel()\n",
    "\n",
    "savepath = os.path.join(\"results\", \"submission_\" + date + \".csv\")\n",
    "data = {\"ID\" : nr, \"Sample\" : sample, \"Label\" : label, \"Predicted\" : y_test_pred}\n",
    "df = pd.DataFrame(data, columns=['ID', 'Sample', 'Label', 'Predicted'])\n",
    "df.to_csv(savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
