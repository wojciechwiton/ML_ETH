{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math, getopt, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate n_arr (almost) equal-sized arrays from arr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(arr, n_arr):\n",
    "    n_elems = int(math.ceil(len(arr)/n_arr))\n",
    "    return [arr[i:i + n_elems] for i in range(0, len(arr), n_elems)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load images from file being in defined path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, file):\n",
    "    filepath = os.path.join(path, file)\n",
    "    img = nib.load(filepath)\n",
    "    img_data = img.get_data()\n",
    "    img_data = img_data[:,:,:,0]\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to delete all zero vectors from image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_data, x_zero, y_zero, z_zero):\n",
    "    img_data = np.delete(img_data, np.where(x_zero==1), axis=0)\n",
    "    img_data = np.delete(img_data, np.where(y_zero==1), axis=1)\n",
    "    img_data = np.delete(img_data, np.where(z_zero==1), axis=2)\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_histograms(path, file, x_zero, y_zero, z_zero, x_pix, y_pix, z_pix):\n",
    "\n",
    "    img_data = load_image(path, file)\n",
    "    preprocess_image(img_data, x_zero, y_zero, z_zero)\n",
    "\n",
    "    for j in range(len(x_pix)):\n",
    "        x_range = x_pix[j]\n",
    "\n",
    "        for k in range(len(y_pix)):\n",
    "            y_range = y_pix[k]\n",
    "\n",
    "            for l in range (len(z_pix)):\n",
    "                z_range = z_pix[l]\n",
    "\n",
    "                temp = np.ix_(x_range, y_range, z_range)\n",
    "                img_part = img_data[temp]\n",
    "                hist, bin_edges = np.histogram(img_part, range=(1e-5,img_data.max()), bins=bins)\n",
    "\n",
    "                if (j,k,l)==(0,0,0):\n",
    "                    part_array = hist\n",
    "                else:\n",
    "                    part_array = np.hstack((part_array, hist))\n",
    "                    \n",
    "    return part_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate histograms for train and test data. For the first image the optimal number of bins is calculated automatically through 'auto' statement. Then the same number of bins is used for all other images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_arrays(x_parts=1, y_parts=1, z_parts=1, bins=10):\n",
    "    \n",
    "    img_data = load_image(src_train_path, \"train_1.nii\")\n",
    "    \n",
    "    #Looking for zero vectors in array\n",
    "    x_zero = np.all(img_data==0, axis=(1,2))\n",
    "    y_zero = np.all(img_data==0, axis=(0,2))\n",
    "    z_zero = np.all(img_data==0, axis=(0,1))\n",
    "    \n",
    "    preprocess_image(img_data, x_zero, y_zero, z_zero)\n",
    "    \n",
    "    (x_dim, y_dim, z_dim) = img_data.shape\n",
    "    x_pix, y_pix, z_pix = chunks(range(x_dim),x_parts), chunks(range(y_dim),y_parts), chunks(range(z_dim),z_parts)\n",
    "    \n",
    "    for i, file in enumerate(train_names):\n",
    "        part_array = count_histograms(src_train_path, file, x_zero, y_zero, z_zero, x_pix, y_pix, z_pix)\n",
    "        \n",
    "        if i == 0:\n",
    "            train_array = part_array\n",
    "        else:\n",
    "            train_array = np.vstack((train_array, part_array))\n",
    "            \n",
    "    for i, file in enumerate(test_names):\n",
    "        part_array = count_histograms(src_test_path, file, x_zero, y_zero, z_zero, x_pix, y_pix, z_pix)\n",
    "        \n",
    "        if i == 0:\n",
    "            test_array = part_array\n",
    "        else:\n",
    "            test_array = np.vstack((test_array, part_array))\n",
    "        \n",
    "    return train_array, test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining paths to .nii files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_path = os.path.join(os.getcwd(), \"data\", \"set_train\")\n",
    "src_test_path = os.path.join(os.getcwd(), \"data\", \"set_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting names of .nii files and sorting them in natural way, so that the order is the same as in the targets.csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = os.path.join(src_train_path, \"*.nii\")\n",
    "train_paths = (glob.glob(train_filepaths))\n",
    "train_names = [os.path.basename(x) for x in train_paths]\n",
    "train_names = natsorted(train_names)\n",
    "\n",
    "test_filepaths = os.path.join(src_test_path, \"*.nii\")\n",
    "test_paths = (glob.glob(test_filepaths))\n",
    "test_names = [os.path.basename(x) for x in test_paths]\n",
    "test_names = natsorted(test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating X (all training samples derived from given training images), X_test_submission (test samples derived from given test images) and y (targets for given training images) matrices. Data from X matrix are split into training and test data for local algorithm validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_parts, y_parts, z_parts, bins = 2, 2, 2, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = histogram_arrays(x_parts, y_parts, z_parts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"targets.csv\", header=None)\n",
    "y = y.values.squeeze()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data and PCA decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20\n",
    "\n",
    "pipeline = make_pipeline(Normalizer(), PCA(n_components=n_components)).fit(X_train.astype(np.float64))\n",
    "\n",
    "X_train = pipeline.transform(X_train.astype(np.float64))\n",
    "X_valid = pipeline.transform(X_valid.astype(np.float64))\n",
    "X_test = pipeline.transform(X_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer = Normalizer().fit(X_train.astype(np.float64))\n",
    "# X_train = normalizer.transform(X_train.astype(np.float64))\n",
    "# X_valid = normalizer.transform(X_valid.astype(np.float64))\n",
    "# X_test = normalizer.transform(X_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR with cross-validation used to fit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "svr = SVR(kernel='rbf')\n",
    "clf = GridSearchCV(svr, param_grid, n_jobs=2)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train)\n",
    "\n",
    "print(scores)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating algorithm by calculating mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "y_train_pred = np.around(y_train_pred)\n",
    "y_train_pred = y_train_pred.astype(int)\n",
    "\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "y_valid_pred = np.around(y_valid_pred)\n",
    "y_valid_pred = y_valid_pred.astype(int)\n",
    "\n",
    "train_error = mean_squared_error(y_train, y_train_pred)\n",
    "valid_error = mean_squared_error(y_valid, y_valid_pred)\n",
    "print(train_error)\n",
    "print(valid_error)\n",
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"Train error: \" + str(train_error))\n",
    "    text_file.write(\"Test error: \" + str(valid_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving scores obtained from original test images to produce file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_pred = np.around(y_test_pred)\n",
    "y_test_pred = y_test_pred.astype(int)\n",
    "\n",
    "nr = np.arange(1,139)\n",
    "\n",
    "df = pd.DataFrame({\"ID\" : nr, \"Prediction\" : y_test_pred})\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
